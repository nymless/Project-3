{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–±—â–∏–µ —É—Ç–∏–ª–∏—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = Path('imgs/finetune_gpt/')\n",
    "DATA_PATH = Path('data/finetune_gpt/')\n",
    "\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "def seed_all(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "[Huggingface](https://huggingface.co/datasets/d0rj/geo-reviews-dataset-2023?row=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c2b60583e84971b0d7b9f78bdd84c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "dataset = load_dataset(\"d0rj/geo-reviews-dataset-2023\", cache_dir=DATA_PATH / 'model_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns in the data set: (500000, 5)\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ DataFrame\n",
    "data_df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "print(\"Number of rows and columns in the data set:\", data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>name_ru</th>\n",
       "      <th>rating</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥, —É–ª. –ú–æ—Å–∫–æ–≤—Å–∫–∞—è / —É–ª. –í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫...</td>\n",
       "      <td>–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–≤–∞—Ä—Ç–∞–ª</td>\n",
       "      <td>3</td>\n",
       "      <td>–ñ–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å</td>\n",
       "      <td>–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–≤–∞—Ä—Ç–∞–ª 2.\\n–®—É–º–Ω–æ : –ª–µ—Ç–æ–º –ø–æ –Ω–æ—á–∞–º ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –≠–ª–µ–∫—Ç—Ä–æ—Å—Ç–∞–ª—å, –ø—Ä–æ—Å–ø–µ–∫—Ç –õ–µ–Ω...</td>\n",
       "      <td>–ü—Ä–æ–¥—É–∫—Ç—ã –ï—Ä–º–æ–ª–∏–Ω–æ</td>\n",
       "      <td>5</td>\n",
       "      <td>–ú–∞–≥–∞–∑–∏–Ω –ø—Ä–æ–¥—É–∫—Ç–æ–≤;–ü—Ä–æ–¥—É–∫—Ç—ã –≥–ª—É–±–æ–∫–æ–π –∑–∞–º–æ—Ä–æ–∑–∫–∏;...</td>\n",
       "      <td>–ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ç—å –º–∞–≥–∞–∑–∏–Ω–æ–≤ –≤ –æ–±—â–µ–º, —Ö–æ—Ä–æ—à–∏–π ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä, –ü—Ä–∏–∫—É–±–∞–Ω—Å–∫–∏–π –≤–Ω—É—Ç—Ä–∏–≥–æ—Ä–æ–¥—Å–∫–æ–π –æ–∫—Ä—É–≥,...</td>\n",
       "      <td>LimeFit</td>\n",
       "      <td>1</td>\n",
       "      <td>–§–∏—Ç–Ω–µ—Å-–∫–ª—É–±</td>\n",
       "      <td>–ù–µ –∑–Ω–∞—é —Å–º—É—Ç—è—Ç –ª–∏ –∫–æ–≥–æ-—Ç–æ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞, –Ω–æ —è...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –ø—Ä–æ—Å–ø–µ–∫—Ç –≠–Ω–≥–µ–ª—å—Å–∞, 111, –∫–æ—Ä–ø. 1</td>\n",
       "      <td>Snow-Express</td>\n",
       "      <td>4</td>\n",
       "      <td>–ü—É–Ω–∫—Ç –ø—Ä–æ–∫–∞—Ç–∞;–ü—Ä–æ–∫–∞—Ç –≤–µ–ª–æ—Å–∏–ø–µ–¥–æ–≤;–°–∞–ø—Å—ë—Ä—Ñ–∏–Ω–≥</td>\n",
       "      <td>–•–æ—Ä–æ—à–∏–µ —É—Å–ª–æ–≤–∏—è –∞—Ä–µ–Ω–¥—ã. \\n–î—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–µ—Ä—Å–æ–Ω–∞–ª...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–¢–≤–µ—Ä—å, –í–æ–ª–æ–∫–æ–ª–∞–º—Å–∫–∏–π –ø—Ä–æ—Å–ø–µ–∫—Ç, 39</td>\n",
       "      <td>–°—Ç—É–¥–∏—è Beauty Brow</td>\n",
       "      <td>5</td>\n",
       "      <td>–°–∞–ª–æ–Ω –∫—Ä–∞—Å–æ—Ç—ã;–í–∏–∑–∞–∂–∏—Å—Ç—ã, —Å—Ç–∏–ª–∏—Å—Ç—ã;–°–∞–ª–æ–Ω –±—Ä–æ–≤–µ–π...</td>\n",
       "      <td>–¢–æ–ø –º–∞—Å—Ç–µ—Ä –ê–Ω–≥–µ–ª–∏–Ω–∞ —Ç–æ–ø –≤–æ –≤—Å–µ—Ö —Å–º—ã—Å–ª–∞—Ö ) –ù–µ–º–Ω...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             address             name_ru  \\\n",
       "0  –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥, —É–ª. –ú–æ—Å–∫–æ–≤—Å–∫–∞—è / —É–ª. –í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫...  –ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–≤–∞—Ä—Ç–∞–ª   \n",
       "1  –ú–æ—Å–∫–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å, –≠–ª–µ–∫—Ç—Ä–æ—Å—Ç–∞–ª—å, –ø—Ä–æ—Å–ø–µ–∫—Ç –õ–µ–Ω...   –ü—Ä–æ–¥—É–∫—Ç—ã –ï—Ä–º–æ–ª–∏–Ω–æ   \n",
       "2  –ö—Ä–∞—Å–Ω–æ–¥–∞—Ä, –ü—Ä–∏–∫—É–±–∞–Ω—Å–∫–∏–π –≤–Ω—É—Ç—Ä–∏–≥–æ—Ä–æ–¥—Å–∫–æ–π –æ–∫—Ä—É–≥,...             LimeFit   \n",
       "3   –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –ø—Ä–æ—Å–ø–µ–∫—Ç –≠–Ω–≥–µ–ª—å—Å–∞, 111, –∫–æ—Ä–ø. 1        Snow-Express   \n",
       "4                  –¢–≤–µ—Ä—å, –í–æ–ª–æ–∫–æ–ª–∞–º—Å–∫–∏–π –ø—Ä–æ—Å–ø–µ–∫—Ç, 39  –°—Ç—É–¥–∏—è Beauty Brow   \n",
       "\n",
       "   rating                                            rubrics  \\\n",
       "0       3                                     –ñ–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å   \n",
       "1       5  –ú–∞–≥–∞–∑–∏–Ω –ø—Ä–æ–¥—É–∫—Ç–æ–≤;–ü—Ä–æ–¥—É–∫—Ç—ã –≥–ª—É–±–æ–∫–æ–π –∑–∞–º–æ—Ä–æ–∑–∫–∏;...   \n",
       "2       1                                        –§–∏—Ç–Ω–µ—Å-–∫–ª—É–±   \n",
       "3       4        –ü—É–Ω–∫—Ç –ø—Ä–æ–∫–∞—Ç–∞;–ü—Ä–æ–∫–∞—Ç –≤–µ–ª–æ—Å–∏–ø–µ–¥–æ–≤;–°–∞–ø—Å—ë—Ä—Ñ–∏–Ω–≥   \n",
       "4       5  –°–∞–ª–æ–Ω –∫—Ä–∞—Å–æ—Ç—ã;–í–∏–∑–∞–∂–∏—Å—Ç—ã, —Å—Ç–∏–ª–∏—Å—Ç—ã;–°–∞–ª–æ–Ω –±—Ä–æ–≤–µ–π...   \n",
       "\n",
       "                                                text  \n",
       "0  –ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–≤–∞—Ä—Ç–∞–ª 2.\\n–®—É–º–Ω–æ : –ª–µ—Ç–æ–º –ø–æ –Ω–æ—á–∞–º ...  \n",
       "1  –ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ç—å –º–∞–≥–∞–∑–∏–Ω–æ–≤ –≤ –æ–±—â–µ–º, —Ö–æ—Ä–æ—à–∏–π ...  \n",
       "2  –ù–µ –∑–Ω–∞—é —Å–º—É—Ç—è—Ç –ª–∏ –∫–æ–≥–æ-—Ç–æ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞, –Ω–æ —è...  \n",
       "3  –•–æ—Ä–æ—à–∏–µ —É—Å–ª–æ–≤–∏—è –∞—Ä–µ–Ω–¥—ã. \\n–î—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø–µ—Ä—Å–æ–Ω–∞–ª...  \n",
       "4  –¢–æ–ø –º–∞—Å—Ç–µ—Ä –ê–Ω–≥–µ–ª–∏–Ω–∞ —Ç–æ–ø –≤–æ –≤—Å–µ—Ö —Å–º—ã—Å–ª–∞—Ö ) –ù–µ–º–Ω...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500000 entries, 0 to 499999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   address  500000 non-null  object\n",
      " 1   name_ru  499030 non-null  object\n",
      " 2   rating   500000 non-null  int64 \n",
      " 3   rubrics  500000 non-null  object\n",
      " 4   text     500000 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 19.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–≤–∞—Ä—Ç–∞–ª 2. –®—É–º–Ω–æ : –ª–µ—Ç–æ–º –ø–æ –Ω–æ—á–∞–º –¥–∏–∫–∏–µ –≥–æ–Ω–∫–∏. –ì—Ä—è–∑–Ω–æ : –∫—Ä—É–≥–æ–º —Å—Ç—Ä–æ–π–∫–∏, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ—Ç–∫—Ä—ã—Ç—å –æ–∫–Ω–∞ (16 —ç—Ç–∞–∂! ), –≤–µ—á–Ω–æ –ø–æ —Ä–∞–π–æ–Ω—É –ª–µ—Ç–∞–µ—Ç –º—É—Å–æ—Ä. –î–µ—Ç—Å–∫–∏–µ –ø–ª–æ—â–∞–¥–∫–∏ —É–±–æ–≥–∏–µ, –Ω–∞ –±–æ–ª—å—à–æ–π –ø–ª–æ—â–∞–¥–∏ –æ–¥–Ω–æ—Ç–∏–ø–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –û—á–µ–Ω—å –¥–æ—Ä–æ–≥–∞—è –∫–æ–º–º—É–Ω–∞–ª–∫–∞. –ß–∞—Å—Ç–æ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–∂–∞—Ä–Ω–∞—è —Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü–∏—è. –ñ–∏–ª—å—Ü—ã —É–∂–µ –Ω–µ —Ä–µ–∞–≥–∏—Ä—É—é—Ç. –í —ç—Ç–æ –≤—Ä–µ–º—è, –æ–±—ã—á–Ω–æ –æ–∫–æ–ª–æ —á–∞—Å–∞, –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç –ª–∏—Ñ—Ç—ã. –ò–∑ –ø–ª—é—Å–æ–≤ - –æ—Ç–ª–∏—á–Ω–∞—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∫–∞ –∫–≤–∞—Ä—Ç–∏—Ä ( –ú–æ—Å–∫–æ–≤—Å–∫–∞—è 194 ), –Ω–∞ –º–æ–π –≤–∑–≥–ª—è–¥. –†–µ–º–æ–Ω—Ç –æ—Ç –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–∞ –Ω–∞ 3-. –û–∫–Ω–∞ –≤–æ–æ–±—â–µ –∂—É—Ç—å - –≤–º–µ—Å—Ç–æ –≤–µ–Ω—Ç–∏–ª—è—Ü–∏–∏. –ü–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é —Ü–µ–Ω–∞/–∫–∞—á–µ—Å—Ç–≤–æ - 3.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_data = data_df.dropna(subset=['text', 'name_ru', 'rating'])\n",
    "work_data = work_data.drop_duplicates(subset=['text']).reset_index(drop=True)\n",
    "work_data['text'] = work_data['text'].str.replace('\\\\n', ' ')\n",
    "work_data = work_data[:50000]\n",
    "work_data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   address  50000 non-null  object\n",
      " 1   name_ru  50000 non-null  object\n",
      " 2   rating   50000 non-null  int64 \n",
      " 3   rubrics  50000 non-null  object\n",
      " 4   text     50000 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "work_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_name_ru = work_data['name_ru'].unique().tolist()\n",
    "\n",
    "unique_rubrics = work_data['rubrics'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ú–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from pathlib import Path\n",
    "\n",
    "class FineTuner:\n",
    "    def __init__(self, \n",
    "                 model_name='ai-forever/rugpt3small_based_on_gpt2', \n",
    "                 cache_dir='model_cache',\n",
    "                 data_path=DATA_PATH):\n",
    "        self.data_path = Path(data_path)\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name, cache_dir=str(self.data_path / cache_dir))\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir=str(self.data_path / cache_dir))\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"\n",
    "        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "        \"\"\"\n",
    "        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø—Ä–æ–±–ª–µ–º—ã –∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "        df['input'] = df.apply(\n",
    "            lambda row: f\"<name_ru> {row['name_ru']} <rubrics> {row['rubrics']} <rating> {row['rating']} {self.tokenizer.eos_token}\", axis=1\n",
    "            )\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫ —Ü–µ–ª–µ–≤–æ–º—É —Ç–µ–∫—Å—Ç—É —Ç–æ–∫–µ–Ω–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è —Å—Ç—Ä–æ–∫–∏\n",
    "        df['output'] = df.apply(lambda row: f\" <text> {row['text']} {self.tokenizer.eos_token}\", axis=1)\n",
    "        \n",
    "        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "        dataset_path = self.data_path / 'train_dataset.txt'\n",
    "        # –ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª\n",
    "        with dataset_path.open('w', encoding='utf-8') as file:\n",
    "            for input_text, target_text in zip(df['input'], df['output']):\n",
    "                file.write(input_text + ' ' + target_text + '\\n')\n",
    "        return dataset_path\n",
    "\n",
    "    def fine_tune(self, \n",
    "                  dataset_path, \n",
    "                  output_name='fine_tuned_model', \n",
    "                  num_train_epochs=4, \n",
    "                  per_device_train_batch_size=4, \n",
    "                  learning_rate=5e-5, \n",
    "                  save_steps=10_000):\n",
    "        \"\"\"\n",
    "        –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.\n",
    "        \"\"\"\n",
    "        train_dataset = TextDataset(\n",
    "            tokenizer=self.tokenizer,\n",
    "            file_path=str(dataset_path),\n",
    "            block_size=256\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer, mlm=False\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(self.data_path / output_name),\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            save_steps=save_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            save_total_limit=2,\n",
    "            logging_dir=str(self.data_path / 'logs'),\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "        self.model.save_pretrained(str(self.data_path / output_name))\n",
    "        self.tokenizer.save_pretrained(str(self.data_path / output_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, model_name='fine_tuned_model', data_path=DATA_PATH):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.\n",
    "        –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø—É—Ç–∏.\n",
    "        \"\"\"\n",
    "        model_path = Path(data_path) / model_name\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(str(model_path))\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(str(model_path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_text(self, \n",
    "                    name_ru: str, \n",
    "                    rubrics: str, \n",
    "                    rating: int,\n",
    "                    max_length=100, \n",
    "                    num_return_sequences=1, \n",
    "                    temperature=1.0, \n",
    "                    top_k=0, \n",
    "                    top_p=1.0, \n",
    "                    do_sample=False):\n",
    "        \"\"\"\n",
    "        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (prompt) –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
    "        \n",
    "        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "        - name_ru: –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏.\n",
    "        - rubrics: –°–ø–∏—Å–æ–∫ —Ä—É–±—Ä–∏–∫, –∫ –∫–æ—Ç–æ—Ä—ã–º –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è.\n",
    "        - rating: –û—Ü–µ–Ω–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n",
    "        - max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\n",
    "        - num_return_sequences: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π.\n",
    "        - temperature: –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤—ã–≤–æ–¥–∞.\n",
    "        - top_k: –ï—Å–ª–∏ –±–æ–ª—å—à–µ 0, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–∫–∏ —Ç–æ–ª—å–∫–æ k –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏.\n",
    "        - top_p: –ï—Å–ª–∏ –º–µ–Ω—å—à–µ 1.0, –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è nucleus sampling.\n",
    "        - do_sample: –ï—Å–ª–∏ True, –≤–∫–ª—é—á–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è.\n",
    "        \"\"\"\n",
    "        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ prompt\n",
    "        prompt_text = f\"<name_ru> {name_ru} <rubrics> {rubrics} <rating> {rating} {self.tokenizer.eos_token} <text> \"\n",
    "        \n",
    "        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –ø—Ä–∏–≥–æ–¥–Ω–æ–º –¥–ª—è –º–æ–¥–µ–ª–∏\n",
    "        encoded_input = self.tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤\n",
    "        outputs = self.model.generate(\n",
    "            encoded_input,\n",
    "            max_length=max_length + len(encoded_input[0]),\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        \n",
    "        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        all_texts = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        \n",
    "        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "        prompt_length = len(self.tokenizer.decode(encoded_input[0], skip_special_tokens=True))\n",
    "        trimmed_texts = [text[prompt_length:] for text in all_texts]\n",
    "        \n",
    "        # –í–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è\n",
    "        return {\n",
    "            \"full_texts\": all_texts,\n",
    "            \"generated_texts\": trimmed_texts\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e64995347a4388b7a3a746069f504c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00ebd5d893d4ba895f38a7bc28dda35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0815f345ccb4c31948db51e9557134f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2727b04589264a1984a2173bbf6bfe59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780ebb66369442b0ba719f9eff8709d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1520a4fd867943eda8c6bf4f24ff8dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/551M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nymle\\Desktop\\Project-3\\venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead5c7194e1c4820b977b3ef186817d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7908, 'grad_norm': 2.468606948852539, 'learning_rate': 4.880816170861938e-05, 'epoch': 0.1}\n",
      "{'loss': 2.6852, 'grad_norm': 2.5022025108337402, 'learning_rate': 4.761632341723875e-05, 'epoch': 0.19}\n",
      "{'loss': 2.6485, 'grad_norm': 2.1006112098693848, 'learning_rate': 4.642448512585813e-05, 'epoch': 0.29}\n",
      "{'loss': 2.6443, 'grad_norm': 2.417471170425415, 'learning_rate': 4.52326468344775e-05, 'epoch': 0.38}\n",
      "{'loss': 2.6262, 'grad_norm': 2.042022705078125, 'learning_rate': 4.4040808543096874e-05, 'epoch': 0.48}\n",
      "{'loss': 2.5911, 'grad_norm': 2.0339229106903076, 'learning_rate': 4.2848970251716244e-05, 'epoch': 0.57}\n",
      "{'loss': 2.5801, 'grad_norm': 2.005763053894043, 'learning_rate': 4.165713196033562e-05, 'epoch': 0.67}\n",
      "{'loss': 2.5962, 'grad_norm': 1.8750629425048828, 'learning_rate': 4.0465293668955e-05, 'epoch': 0.76}\n",
      "{'loss': 2.5823, 'grad_norm': 1.6160539388656616, 'learning_rate': 3.9273455377574375e-05, 'epoch': 0.86}\n",
      "{'loss': 2.5607, 'grad_norm': 1.962394118309021, 'learning_rate': 3.808161708619375e-05, 'epoch': 0.95}\n",
      "{'loss': 2.4654, 'grad_norm': 1.8571257591247559, 'learning_rate': 3.688977879481312e-05, 'epoch': 1.05}\n",
      "{'loss': 2.3954, 'grad_norm': 2.0428998470306396, 'learning_rate': 3.56979405034325e-05, 'epoch': 1.14}\n",
      "{'loss': 2.3812, 'grad_norm': 1.8628681898117065, 'learning_rate': 3.450610221205187e-05, 'epoch': 1.24}\n",
      "{'loss': 2.3993, 'grad_norm': 2.155229091644287, 'learning_rate': 3.3314263920671247e-05, 'epoch': 1.33}\n",
      "{'loss': 2.3871, 'grad_norm': 1.9779056310653687, 'learning_rate': 3.212242562929062e-05, 'epoch': 1.43}\n",
      "{'loss': 2.405, 'grad_norm': 2.1200222969055176, 'learning_rate': 3.0930587337909994e-05, 'epoch': 1.53}\n",
      "{'loss': 2.3898, 'grad_norm': 2.256021499633789, 'learning_rate': 2.9738749046529367e-05, 'epoch': 1.62}\n",
      "{'loss': 2.4123, 'grad_norm': 2.0021932125091553, 'learning_rate': 2.854691075514874e-05, 'epoch': 1.72}\n",
      "{'loss': 2.3793, 'grad_norm': 1.9505411386489868, 'learning_rate': 2.7355072463768118e-05, 'epoch': 1.81}\n",
      "{'loss': 2.3771, 'grad_norm': 2.2053823471069336, 'learning_rate': 2.616323417238749e-05, 'epoch': 1.91}\n",
      "{'loss': 2.3871, 'grad_norm': 1.8860630989074707, 'learning_rate': 2.4971395881006865e-05, 'epoch': 2.0}\n",
      "{'loss': 2.2394, 'grad_norm': 2.4735963344573975, 'learning_rate': 2.3779557589626242e-05, 'epoch': 2.1}\n",
      "{'loss': 2.2511, 'grad_norm': 2.281616687774658, 'learning_rate': 2.2587719298245616e-05, 'epoch': 2.19}\n",
      "{'loss': 2.254, 'grad_norm': 1.7874614000320435, 'learning_rate': 2.139588100686499e-05, 'epoch': 2.29}\n",
      "{'loss': 2.2665, 'grad_norm': 2.3548784255981445, 'learning_rate': 2.0204042715484363e-05, 'epoch': 2.38}\n",
      "{'loss': 2.2599, 'grad_norm': 1.9757534265518188, 'learning_rate': 1.9012204424103737e-05, 'epoch': 2.48}\n",
      "{'loss': 2.2269, 'grad_norm': 2.0777623653411865, 'learning_rate': 1.7820366132723114e-05, 'epoch': 2.57}\n",
      "{'loss': 2.2653, 'grad_norm': 2.199845314025879, 'learning_rate': 1.6628527841342487e-05, 'epoch': 2.67}\n",
      "{'loss': 2.2576, 'grad_norm': 2.092099666595459, 'learning_rate': 1.543668954996186e-05, 'epoch': 2.77}\n",
      "{'loss': 2.246, 'grad_norm': 2.206249952316284, 'learning_rate': 1.4244851258581238e-05, 'epoch': 2.86}\n",
      "{'loss': 2.2687, 'grad_norm': 2.0383777618408203, 'learning_rate': 1.3053012967200612e-05, 'epoch': 2.96}\n",
      "{'loss': 2.2209, 'grad_norm': 1.9993188381195068, 'learning_rate': 1.1861174675819985e-05, 'epoch': 3.05}\n",
      "{'loss': 2.1582, 'grad_norm': 2.1160762310028076, 'learning_rate': 1.066933638443936e-05, 'epoch': 3.15}\n",
      "{'loss': 2.1451, 'grad_norm': 2.3155856132507324, 'learning_rate': 9.477498093058734e-06, 'epoch': 3.24}\n",
      "{'loss': 2.1675, 'grad_norm': 2.093331813812256, 'learning_rate': 8.285659801678108e-06, 'epoch': 3.34}\n",
      "{'loss': 2.1484, 'grad_norm': 2.2656445503234863, 'learning_rate': 7.093821510297482e-06, 'epoch': 3.43}\n",
      "{'loss': 2.1817, 'grad_norm': 2.3360543251037598, 'learning_rate': 5.9019832189168576e-06, 'epoch': 3.53}\n",
      "{'loss': 2.1452, 'grad_norm': 1.8692243099212646, 'learning_rate': 4.710144927536232e-06, 'epoch': 3.62}\n",
      "{'loss': 2.1649, 'grad_norm': 2.248412847518921, 'learning_rate': 3.5183066361556065e-06, 'epoch': 3.72}\n",
      "{'loss': 2.1678, 'grad_norm': 2.2593209743499756, 'learning_rate': 2.3264683447749814e-06, 'epoch': 3.81}\n",
      "{'loss': 2.1628, 'grad_norm': 2.2800819873809814, 'learning_rate': 1.1346300533943554e-06, 'epoch': 3.91}\n",
      "{'train_runtime': 2564.4903, 'train_samples_per_second': 32.714, 'train_steps_per_second': 8.179, 'train_loss': 2.358363596016355, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "finetuner = FineTuner()\n",
    "dataset_path = finetuner.prepare_data(work_data)\n",
    "finetuner.fine_tune(dataset_path, output_name='fine_tuned_model_gpt_2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–µ–¥–∏–∫—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –∫–≤–∞—Ä—Ç–∞–ª',\n",
       " '–ü—Ä–æ–¥—É–∫—Ç—ã –ï—Ä–º–æ–ª–∏–Ω–æ',\n",
       " 'LimeFit',\n",
       " 'Snow-Express',\n",
       " '–°—Ç—É–¥–∏—è Beauty Brow',\n",
       " 'Tele2',\n",
       " '–£ —Ç–µ—â–∏',\n",
       " 'Smoking Park',\n",
       " 'Jinju',\n",
       " 'Kari –ì–ò–ü–ï–†']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_name_ru[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–ñ–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å',\n",
       " '–ú–∞–≥–∞–∑–∏–Ω –ø—Ä–æ–¥—É–∫—Ç–æ–≤;–ü—Ä–æ–¥—É–∫—Ç—ã –≥–ª—É–±–æ–∫–æ–π –∑–∞–º–æ—Ä–æ–∑–∫–∏;–ú–∞–≥–∞–∑–∏–Ω –º—è—Å–∞, –∫–æ–ª–±–∞—Å',\n",
       " '–§–∏—Ç–Ω–µ—Å-–∫–ª—É–±',\n",
       " '–ü—É–Ω–∫—Ç –ø—Ä–æ–∫–∞—Ç–∞;–ü—Ä–æ–∫–∞—Ç –≤–µ–ª–æ—Å–∏–ø–µ–¥–æ–≤;–°–∞–ø—Å—ë—Ä—Ñ–∏–Ω–≥',\n",
       " '–°–∞–ª–æ–Ω –∫—Ä–∞—Å–æ—Ç—ã;–í–∏–∑–∞–∂–∏—Å—Ç—ã, —Å—Ç–∏–ª–∏—Å—Ç—ã;–°–∞–ª–æ–Ω –±—Ä–æ–≤–µ–π –∏ —Ä–µ—Å–Ω–∏—Ü',\n",
       " '–û–ø–µ—Ä–∞—Ç–æ—Ä —Å–æ—Ç–æ–≤–æ–π —Å–≤—è–∑–∏;–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø—Ä–æ–≤–∞–π–¥–µ—Ä',\n",
       " '–ö–∞—Ñ–µ',\n",
       " '–í–µ–π–ø-—à–æ–ø;–ú–∞–≥–∞–∑–∏–Ω —Ç–∞–±–∞–∫–∞ –∏ –∫—É—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–µ–π',\n",
       " '–ö–∞—Ñ–µ;–ö–æ—Ñ–µ–π–Ω—è',\n",
       " '–ú–∞–≥–∞–∑–∏–Ω –æ–±—É–≤–∏;–Æ–≤–µ–ª–∏—Ä–Ω—ã–π –º–∞–≥–∞–∑–∏–Ω;–î–µ—Ç—Å–∫–∏–µ –∏–≥—Ä—É—à–∫–∏ –∏ –∏–≥—Ä—ã']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_rubrics[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1:  –£–∂–∞—Å–Ω—ã–π –º–∞–≥–∞–∑–∏–Ω, –Ω–µ —Å–æ–≤–µ—Ç—É—é —Ç—É–¥–∞ —Ö–æ–¥–∏—Ç—å, –ª—É—á—à–µ —É–∂ –Ω–∞ —É–ª–∏—Ü–µ –∫—É–ø–∏—Ç—å –≤–∫—É—Å–Ω—è—à–µ–∫ –≤ –¥–æ—Ä–æ–≥—É ü§¶üèº‚Äç‚ôÄÔ∏è  –ù–µ —Å–æ–≤–µ—Ç—É—é –Ω–∏–∫–æ–º—É!!  –°–≤–µ–∂–∏–µ –æ–≤–æ—â–∏ –∏ —Ñ—Ä—É–∫—Ç—ã  –Ω–µ –≤—Å–µ–≥–¥–∞  —Å–≤–µ–∂–∏–µ!  –ù–∞ –∫–∞—Å—Å–µ  –æ—á–µ—Ä–µ–¥–∏  –∏ –æ—á–µ—Ä–µ–¥–∏ –Ω–µ –∏–∑ –ø—Ä–æ—Å—Ç—ã—Ö üòÇ  –õ—é–¥–∏  —Ö–æ—Ç—è—Ç –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å  –¥–µ–Ω–µ–≥ –∏ –∫—É–ø–∏—Ç—å  –≤–∫—É—Å–Ω–µ–Ω—å–∫–æ–µ  –Ω–∞ –ø—Ä–∞–∑–¥–Ω–∏–∫–∏ ,  –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∫—É–ø–∏—Ç—å,  –∫–∞–∫ –Ω–∞  —Ä—ã–Ω–∫–µ  ‚úÖ  –¶–µ–Ω—ã –≤—ã—Å–æ–∫–∏–µ  –≤ –º–∞–≥–∞–∑–∏–Ω–µ !  \n",
      "Generated Text 2:  –û—á–µ–Ω—å –Ω–µ–ø—Ä–∏—è—Ç–Ω—ã–π –º–∞–≥–∞–∑–∏–Ω. –û—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ –ø–æ–∫—É–ø–∞—Ç–µ–ª—è–º. –ù–∞ –∫–∞—Å—Å–∞—Ö —Å–∏–¥—è—Ç –º–æ–ª–æ–¥—ã–µ –ª—é–¥–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∏—á–µ–≥–æ –Ω–µ –º–æ–≥—É—Ç —Å–¥–µ–ª–∞—Ç—å, –∫—Ä–æ–º–µ –∫–∞–∫ —Å–ø—Ä–æ—Å–∏—Ç—å. –í –º–∞–≥–∞–∑–∏–Ω–µ —á–∏—Å—Ç–æ, —Ç–æ–≤–∞—Ä –≤—ã–∫–ª–∞–¥—ã–≤–∞—é—Ç –≤ —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫, –Ω–æ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è—é—Ç –µ–≥–æ –Ω–∞ –º–µ—Å—Ç–µ –∏–ª–∏ –Ω–µ—Ç.  –£–∂–∞—Å!  –ü—Ä–∏–≤–æ–∑—è—Ç –≤ –º–∞–≥–∞–∑–∏–Ω –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ –æ–≤–æ—â–∏ –∏ —Ñ—Ä—É–∫—Ç—ã, –∏ –≥–æ–≤–æ—Ä—è—Ç, —á—Ç–æ –æ–Ω–∏ –Ω–µ —Å–≤–µ–∂–∏–µ. –ü—Ä–∏–≤–µ–∑–ª–∏ –Ω–µ —Ç–µ –æ–≤–æ—â–∏, –Ω–µ —Ç–æ –º—è—Å–æ.  \n",
      "Generated Text 3:  –ù–µ –ø–æ–∫—É–ø–∞–π—Ç–µ –∑–¥–µ—Å—å –ø—Ä–æ–¥—É–∫—Ç—ã –ï—Ä–º–æ–ª–∏–Ω–∞. –≠—Ç–æ –Ω–µ –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã.  –ú—è—Å–æ –Ω–µ —Å–≤–µ–∂–µ–µ, –º—è—Å–æ –≥–Ω–∏–ª–æ–µ. –ü—Ä–æ–¥–∞–≤—Ü—ã –≥—Ä—É–±—è—Ç, —á—Ç–æ –Ω–µ –º–æ–≥—É—Ç –Ω–∞–π—Ç–∏ –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π.  \n"
     ]
    }
   ],
   "source": [
    "name_ru = unique_name_ru[1]\n",
    "rubrics = unique_rubrics[1]\n",
    "rating = 1\n",
    "\n",
    "generator = TextGenerator(\n",
    "    model_name='fine_tuned_model_gpt_2',\n",
    "    data_path=DATA_PATH\n",
    ")\n",
    "generated_texts = generator.generate_text(\n",
    "    name_ru=name_ru,\n",
    "    rubrics=rubrics,\n",
    "    rating=rating,\n",
    "    max_length=200,\n",
    "    # num_beams=3 # –µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π \n",
    "    num_return_sequences=3,\n",
    "    do_sample=True,\n",
    "    temperature=0.95,  # –°–ª–µ–≥–∫–∞ —É–º–µ–Ω—å—à–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å\n",
    "    top_k=10,         # –£–º–µ–Ω—å—à–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º—ã—Ö –≤–µ—Ä—Ö–Ω–∏—Ö k —Å–ª–æ–≤\n",
    "    top_p=0.95        # –£–º–µ–Ω—å—à–∞–µ–º \"—è–¥–µ—Ä–Ω–æ—Å—Ç—å\" —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
    ")\n",
    "for i, text in enumerate(generated_texts['generated_texts']):\n",
    "    print(f\"Generated Text {i+1}: {text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
