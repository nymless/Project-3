{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общие утилиты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = Path('imgs/finetune_gpt/')\n",
    "DATA_PATH = Path('data/finetune_gpt/')\n",
    "\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "def seed_all(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных\n",
    "\n",
    "[Huggingface](https://huggingface.co/datasets/d0rj/geo-reviews-dataset-2023?row=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c2b60583e84971b0d7b9f78bdd84c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Загрузка датасета\n",
    "dataset = load_dataset(\"d0rj/geo-reviews-dataset-2023\", cache_dir=DATA_PATH / 'model_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns in the data set: (500000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Преобразование данных в DataFrame\n",
    "data_df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "print(\"Number of rows and columns in the data set:\", data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>name_ru</th>\n",
       "      <th>rating</th>\n",
       "      <th>rubrics</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Екатеринбург, ул. Московская / ул. Волгоградск...</td>\n",
       "      <td>Московский квартал</td>\n",
       "      <td>3</td>\n",
       "      <td>Жилой комплекс</td>\n",
       "      <td>Московский квартал 2.\\nШумно : летом по ночам ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Московская область, Электросталь, проспект Лен...</td>\n",
       "      <td>Продукты Ермолино</td>\n",
       "      <td>5</td>\n",
       "      <td>Магазин продуктов;Продукты глубокой заморозки;...</td>\n",
       "      <td>Замечательная сеть магазинов в общем, хороший ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Краснодар, Прикубанский внутригородской округ,...</td>\n",
       "      <td>LimeFit</td>\n",
       "      <td>1</td>\n",
       "      <td>Фитнес-клуб</td>\n",
       "      <td>Не знаю смутят ли кого-то данные правила, но я...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Санкт-Петербург, проспект Энгельса, 111, корп. 1</td>\n",
       "      <td>Snow-Express</td>\n",
       "      <td>4</td>\n",
       "      <td>Пункт проката;Прокат велосипедов;Сапсёрфинг</td>\n",
       "      <td>Хорошие условия аренды. \\nДружелюбный персонал...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Тверь, Волоколамский проспект, 39</td>\n",
       "      <td>Студия Beauty Brow</td>\n",
       "      <td>5</td>\n",
       "      <td>Салон красоты;Визажисты, стилисты;Салон бровей...</td>\n",
       "      <td>Топ мастер Ангелина топ во всех смыслах ) Немн...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             address             name_ru  \\\n",
       "0  Екатеринбург, ул. Московская / ул. Волгоградск...  Московский квартал   \n",
       "1  Московская область, Электросталь, проспект Лен...   Продукты Ермолино   \n",
       "2  Краснодар, Прикубанский внутригородской округ,...             LimeFit   \n",
       "3   Санкт-Петербург, проспект Энгельса, 111, корп. 1        Snow-Express   \n",
       "4                  Тверь, Волоколамский проспект, 39  Студия Beauty Brow   \n",
       "\n",
       "   rating                                            rubrics  \\\n",
       "0       3                                     Жилой комплекс   \n",
       "1       5  Магазин продуктов;Продукты глубокой заморозки;...   \n",
       "2       1                                        Фитнес-клуб   \n",
       "3       4        Пункт проката;Прокат велосипедов;Сапсёрфинг   \n",
       "4       5  Салон красоты;Визажисты, стилисты;Салон бровей...   \n",
       "\n",
       "                                                text  \n",
       "0  Московский квартал 2.\\nШумно : летом по ночам ...  \n",
       "1  Замечательная сеть магазинов в общем, хороший ...  \n",
       "2  Не знаю смутят ли кого-то данные правила, но я...  \n",
       "3  Хорошие условия аренды. \\nДружелюбный персонал...  \n",
       "4  Топ мастер Ангелина топ во всех смыслах ) Немн...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500000 entries, 0 to 499999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   address  500000 non-null  object\n",
      " 1   name_ru  499030 non-null  object\n",
      " 2   rating   500000 non-null  int64 \n",
      " 3   rubrics  500000 non-null  object\n",
      " 4   text     500000 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 19.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Московский квартал 2. Шумно : летом по ночам дикие гонки. Грязно : кругом стройки, невозможно открыть окна (16 этаж! ), вечно по району летает мусор. Детские площадки убогие, на большой площади однотипные конструкции. Очень дорогая коммуналка. Часто срабатывает пожарная сигнализация. Жильцы уже не реагируют. В это время, обычно около часа, не работают лифты. Из плюсов - отличная планировка квартир ( Московская 194 ), на мой взгляд. Ремонт от застройщика на 3-. Окна вообще жуть - вместо вентиляции. По соотношению цена/качество - 3.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_data = data_df.dropna(subset=['text', 'name_ru', 'rating'])\n",
    "work_data = work_data.drop_duplicates(subset=['text']).reset_index(drop=True)\n",
    "work_data['text'] = work_data['text'].str.replace('\\\\n', ' ')\n",
    "work_data = work_data[:50000]\n",
    "work_data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   address  50000 non-null  object\n",
      " 1   name_ru  50000 non-null  object\n",
      " 2   rating   50000 non-null  int64 \n",
      " 3   rubrics  50000 non-null  object\n",
      " 4   text     50000 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "work_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_name_ru = work_data['name_ru'].unique().tolist()\n",
    "\n",
    "unique_rubrics = work_data['rubrics'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from pathlib import Path\n",
    "\n",
    "class FineTuner:\n",
    "    def __init__(self, \n",
    "                 model_name='ai-forever/rugpt3small_based_on_gpt2', \n",
    "                 cache_dir='model_cache',\n",
    "                 data_path=DATA_PATH):\n",
    "        self.data_path = Path(data_path)\n",
    "        \n",
    "        # Инициализация токенизатора и модели\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name, cache_dir=str(self.data_path / cache_dir))\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir=str(self.data_path / cache_dir))\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"\n",
    "        Подготовка данных для обучения\n",
    "        \"\"\"\n",
    "        # Объединение типа проблемы и исходного текста в одну строку входных данных\n",
    "        df['input'] = df.apply(\n",
    "            lambda row: f\"<name_ru> {row['name_ru']} <rubrics> {row['rubrics']} <rating> {row['rating']} {self.tokenizer.eos_token}\", axis=1\n",
    "            )\n",
    "        \n",
    "        # Добавление к целевому тексту токена окончания строки\n",
    "        df['output'] = df.apply(lambda row: f\" <text> {row['text']} {self.tokenizer.eos_token}\", axis=1)\n",
    "        \n",
    "        # Подготовка пути для сохранения данных\n",
    "        dataset_path = self.data_path / 'train_dataset.txt'\n",
    "        # Запись данных в файл\n",
    "        with dataset_path.open('w', encoding='utf-8') as file:\n",
    "            for input_text, target_text in zip(df['input'], df['output']):\n",
    "                file.write(input_text + ' ' + target_text + '\\n')\n",
    "        return dataset_path\n",
    "\n",
    "    def fine_tune(self, \n",
    "                  dataset_path, \n",
    "                  output_name='fine_tuned_model', \n",
    "                  num_train_epochs=4, \n",
    "                  per_device_train_batch_size=4, \n",
    "                  learning_rate=5e-5, \n",
    "                  save_steps=10_000):\n",
    "        \"\"\"\n",
    "        Дообучение модели на заданном датасете.\n",
    "        \"\"\"\n",
    "        train_dataset = TextDataset(\n",
    "            tokenizer=self.tokenizer,\n",
    "            file_path=str(dataset_path),\n",
    "            block_size=256\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer, mlm=False\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(self.data_path / output_name),\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            save_steps=save_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            save_total_limit=2,\n",
    "            logging_dir=str(self.data_path / 'logs'),\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        # Сохранение обученной модели и токенизатора\n",
    "        self.model.save_pretrained(str(self.data_path / output_name))\n",
    "        self.tokenizer.save_pretrained(str(self.data_path / output_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, model_name='fine_tuned_model', data_path=DATA_PATH):\n",
    "        \"\"\"\n",
    "        Инициализация модели и токенизатора.\n",
    "        Загружаем модель и токенизатор из указанного пути.\n",
    "        \"\"\"\n",
    "        model_path = Path(data_path) / model_name\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(str(model_path))\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(str(model_path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_text(self, \n",
    "                    name_ru: str, \n",
    "                    rubrics: str, \n",
    "                    rating: int,\n",
    "                    max_length=100, \n",
    "                    num_return_sequences=1, \n",
    "                    temperature=1.0, \n",
    "                    top_k=0, \n",
    "                    top_p=1.0, \n",
    "                    do_sample=False):\n",
    "        \"\"\"\n",
    "        Генерация текста на основе заданного начального текста (prompt) и параметров.\n",
    "        \n",
    "        Параметры:\n",
    "        - name_ru: Название организации.\n",
    "        - rubrics: Список рубрик, к которым относится организация.\n",
    "        - rating: Оценка пользователя.\n",
    "        - max_length: Максимальная длина сгенерированного текста.\n",
    "        - num_return_sequences: Количество возвращаемых последовательностей.\n",
    "        - temperature: Контролирует разнообразие вывода.\n",
    "        - top_k: Если больше 0, ограничивает количество слов для выборки только k наиболее вероятными словами.\n",
    "        - top_p: Если меньше 1.0, применяется nucleus sampling.\n",
    "        - do_sample: Если True, включает случайную выборку для увеличения разнообразия.\n",
    "        \"\"\"\n",
    "        # Формирование prompt\n",
    "        prompt_text = f\"<name_ru> {name_ru} <rubrics> {rubrics} <rating> {rating} {self.tokenizer.eos_token} <text> \"\n",
    "        \n",
    "        # Кодирование текста в формате, пригодном для модели\n",
    "        encoded_input = self.tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "        \n",
    "        # Генерация текстов\n",
    "        outputs = self.model.generate(\n",
    "            encoded_input,\n",
    "            max_length=max_length + len(encoded_input[0]),\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        \n",
    "        # Декодирование результатов\n",
    "        all_texts = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        \n",
    "        # Удаление входных данных из текстов\n",
    "        prompt_length = len(self.tokenizer.decode(encoded_input[0], skip_special_tokens=True))\n",
    "        trimmed_texts = [text[prompt_length:] for text in all_texts]\n",
    "        \n",
    "        # Возврат результатов в виде словаря\n",
    "        return {\n",
    "            \"full_texts\": all_texts,\n",
    "            \"generated_texts\": trimmed_texts\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e64995347a4388b7a3a746069f504c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00ebd5d893d4ba895f38a7bc28dda35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0815f345ccb4c31948db51e9557134f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2727b04589264a1984a2173bbf6bfe59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780ebb66369442b0ba719f9eff8709d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1520a4fd867943eda8c6bf4f24ff8dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/551M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nymle\\Desktop\\Project-3\\venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead5c7194e1c4820b977b3ef186817d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7908, 'grad_norm': 2.468606948852539, 'learning_rate': 4.880816170861938e-05, 'epoch': 0.1}\n",
      "{'loss': 2.6852, 'grad_norm': 2.5022025108337402, 'learning_rate': 4.761632341723875e-05, 'epoch': 0.19}\n",
      "{'loss': 2.6485, 'grad_norm': 2.1006112098693848, 'learning_rate': 4.642448512585813e-05, 'epoch': 0.29}\n",
      "{'loss': 2.6443, 'grad_norm': 2.417471170425415, 'learning_rate': 4.52326468344775e-05, 'epoch': 0.38}\n",
      "{'loss': 2.6262, 'grad_norm': 2.042022705078125, 'learning_rate': 4.4040808543096874e-05, 'epoch': 0.48}\n",
      "{'loss': 2.5911, 'grad_norm': 2.0339229106903076, 'learning_rate': 4.2848970251716244e-05, 'epoch': 0.57}\n",
      "{'loss': 2.5801, 'grad_norm': 2.005763053894043, 'learning_rate': 4.165713196033562e-05, 'epoch': 0.67}\n",
      "{'loss': 2.5962, 'grad_norm': 1.8750629425048828, 'learning_rate': 4.0465293668955e-05, 'epoch': 0.76}\n",
      "{'loss': 2.5823, 'grad_norm': 1.6160539388656616, 'learning_rate': 3.9273455377574375e-05, 'epoch': 0.86}\n",
      "{'loss': 2.5607, 'grad_norm': 1.962394118309021, 'learning_rate': 3.808161708619375e-05, 'epoch': 0.95}\n",
      "{'loss': 2.4654, 'grad_norm': 1.8571257591247559, 'learning_rate': 3.688977879481312e-05, 'epoch': 1.05}\n",
      "{'loss': 2.3954, 'grad_norm': 2.0428998470306396, 'learning_rate': 3.56979405034325e-05, 'epoch': 1.14}\n",
      "{'loss': 2.3812, 'grad_norm': 1.8628681898117065, 'learning_rate': 3.450610221205187e-05, 'epoch': 1.24}\n",
      "{'loss': 2.3993, 'grad_norm': 2.155229091644287, 'learning_rate': 3.3314263920671247e-05, 'epoch': 1.33}\n",
      "{'loss': 2.3871, 'grad_norm': 1.9779056310653687, 'learning_rate': 3.212242562929062e-05, 'epoch': 1.43}\n",
      "{'loss': 2.405, 'grad_norm': 2.1200222969055176, 'learning_rate': 3.0930587337909994e-05, 'epoch': 1.53}\n",
      "{'loss': 2.3898, 'grad_norm': 2.256021499633789, 'learning_rate': 2.9738749046529367e-05, 'epoch': 1.62}\n",
      "{'loss': 2.4123, 'grad_norm': 2.0021932125091553, 'learning_rate': 2.854691075514874e-05, 'epoch': 1.72}\n",
      "{'loss': 2.3793, 'grad_norm': 1.9505411386489868, 'learning_rate': 2.7355072463768118e-05, 'epoch': 1.81}\n",
      "{'loss': 2.3771, 'grad_norm': 2.2053823471069336, 'learning_rate': 2.616323417238749e-05, 'epoch': 1.91}\n",
      "{'loss': 2.3871, 'grad_norm': 1.8860630989074707, 'learning_rate': 2.4971395881006865e-05, 'epoch': 2.0}\n",
      "{'loss': 2.2394, 'grad_norm': 2.4735963344573975, 'learning_rate': 2.3779557589626242e-05, 'epoch': 2.1}\n",
      "{'loss': 2.2511, 'grad_norm': 2.281616687774658, 'learning_rate': 2.2587719298245616e-05, 'epoch': 2.19}\n",
      "{'loss': 2.254, 'grad_norm': 1.7874614000320435, 'learning_rate': 2.139588100686499e-05, 'epoch': 2.29}\n",
      "{'loss': 2.2665, 'grad_norm': 2.3548784255981445, 'learning_rate': 2.0204042715484363e-05, 'epoch': 2.38}\n",
      "{'loss': 2.2599, 'grad_norm': 1.9757534265518188, 'learning_rate': 1.9012204424103737e-05, 'epoch': 2.48}\n",
      "{'loss': 2.2269, 'grad_norm': 2.0777623653411865, 'learning_rate': 1.7820366132723114e-05, 'epoch': 2.57}\n",
      "{'loss': 2.2653, 'grad_norm': 2.199845314025879, 'learning_rate': 1.6628527841342487e-05, 'epoch': 2.67}\n",
      "{'loss': 2.2576, 'grad_norm': 2.092099666595459, 'learning_rate': 1.543668954996186e-05, 'epoch': 2.77}\n",
      "{'loss': 2.246, 'grad_norm': 2.206249952316284, 'learning_rate': 1.4244851258581238e-05, 'epoch': 2.86}\n",
      "{'loss': 2.2687, 'grad_norm': 2.0383777618408203, 'learning_rate': 1.3053012967200612e-05, 'epoch': 2.96}\n",
      "{'loss': 2.2209, 'grad_norm': 1.9993188381195068, 'learning_rate': 1.1861174675819985e-05, 'epoch': 3.05}\n",
      "{'loss': 2.1582, 'grad_norm': 2.1160762310028076, 'learning_rate': 1.066933638443936e-05, 'epoch': 3.15}\n",
      "{'loss': 2.1451, 'grad_norm': 2.3155856132507324, 'learning_rate': 9.477498093058734e-06, 'epoch': 3.24}\n",
      "{'loss': 2.1675, 'grad_norm': 2.093331813812256, 'learning_rate': 8.285659801678108e-06, 'epoch': 3.34}\n",
      "{'loss': 2.1484, 'grad_norm': 2.2656445503234863, 'learning_rate': 7.093821510297482e-06, 'epoch': 3.43}\n",
      "{'loss': 2.1817, 'grad_norm': 2.3360543251037598, 'learning_rate': 5.9019832189168576e-06, 'epoch': 3.53}\n",
      "{'loss': 2.1452, 'grad_norm': 1.8692243099212646, 'learning_rate': 4.710144927536232e-06, 'epoch': 3.62}\n",
      "{'loss': 2.1649, 'grad_norm': 2.248412847518921, 'learning_rate': 3.5183066361556065e-06, 'epoch': 3.72}\n",
      "{'loss': 2.1678, 'grad_norm': 2.2593209743499756, 'learning_rate': 2.3264683447749814e-06, 'epoch': 3.81}\n",
      "{'loss': 2.1628, 'grad_norm': 2.2800819873809814, 'learning_rate': 1.1346300533943554e-06, 'epoch': 3.91}\n",
      "{'train_runtime': 2564.4903, 'train_samples_per_second': 32.714, 'train_steps_per_second': 8.179, 'train_loss': 2.358363596016355, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "finetuner = FineTuner()\n",
    "dataset_path = finetuner.prepare_data(work_data)\n",
    "finetuner.fine_tune(dataset_path, output_name='fine_tuned_model_gpt_2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предикт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Московский квартал',\n",
       " 'Продукты Ермолино',\n",
       " 'LimeFit',\n",
       " 'Snow-Express',\n",
       " 'Студия Beauty Brow',\n",
       " 'Tele2',\n",
       " 'У тещи',\n",
       " 'Smoking Park',\n",
       " 'Jinju',\n",
       " 'Kari ГИПЕР']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_name_ru[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Жилой комплекс',\n",
       " 'Магазин продуктов;Продукты глубокой заморозки;Магазин мяса, колбас',\n",
       " 'Фитнес-клуб',\n",
       " 'Пункт проката;Прокат велосипедов;Сапсёрфинг',\n",
       " 'Салон красоты;Визажисты, стилисты;Салон бровей и ресниц',\n",
       " 'Оператор сотовой связи;Интернет-провайдер',\n",
       " 'Кафе',\n",
       " 'Вейп-шоп;Магазин табака и курительных принадлежностей',\n",
       " 'Кафе;Кофейня',\n",
       " 'Магазин обуви;Ювелирный магазин;Детские игрушки и игры']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_rubrics[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1:  Ужасный магазин, не советую туда ходить, лучше уж на улице купить вкусняшек в дорогу 🤦🏼‍♀️  Не советую никому!!  Свежие овощи и фрукты  не всегда  свежие!  На кассе  очереди  и очереди не из простых 😂  Люди  хотят заработать  денег и купить  вкусненькое  на праздники ,  а не просто купить,  как на  рынке  ✅  Цены высокие  в магазине !  \n",
      "Generated Text 2:  Очень неприятный магазин. Отвратительное отношение к покупателям. На кассах сидят молодые люди, которые ничего не могут сделать, кроме как спросить. В магазине чисто, товар выкладывают в холодильник, но не проверяют его на месте или нет.  Ужас!  Привозят в магазин замороженные овощи и фрукты, и говорят, что они не свежие. Привезли не те овощи, не то мясо.  \n",
      "Generated Text 3:  Не покупайте здесь продукты Ермолина. Это не натуральные продукты.  Мясо не свежее, мясо гнилое. Продавцы грубят, что не могут найти покупателей.  \n"
     ]
    }
   ],
   "source": [
    "name_ru = unique_name_ru[1]\n",
    "rubrics = unique_rubrics[1]\n",
    "rating = 1\n",
    "\n",
    "generator = TextGenerator(\n",
    "    model_name='fine_tuned_model_gpt_2',\n",
    "    data_path=DATA_PATH\n",
    ")\n",
    "generated_texts = generator.generate_text(\n",
    "    name_ru=name_ru,\n",
    "    rubrics=rubrics,\n",
    "    rating=rating,\n",
    "    max_length=200,\n",
    "    # num_beams=3 # если несколько последовательностей \n",
    "    num_return_sequences=3,\n",
    "    do_sample=True,\n",
    "    temperature=0.95,  # Слегка уменьшаем уверенность\n",
    "    top_k=10,         # Уменьшаем количество рассматриваемых верхних k слов\n",
    "    top_p=0.95        # Уменьшаем \"ядерность\" распределения\n",
    ")\n",
    "for i, text in enumerate(generated_texts['generated_texts']):\n",
    "    print(f\"Generated Text {i+1}: {text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
