{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дообучение модели Gemma-2-2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Дообучим предобученную большую языковую модель `Gemma-2-2b` от компании `Google`.\n",
    "* Модель `Gemma-2-2b` построена на улучшенной архитектуре GPT и имеет 2 млрд параметров. Ссылка на модель: <https://huggingface.co/google/gemma-2-2b>\n",
    "* Для обучения используем библиотеку `transformers` от `Hugging Face` и `PyTorch` в качестве backend.\n",
    "* Ссылка на данные: <https://github.com/yandex/geo-reviews-dataset-2023>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
    "                          DataCollatorForLanguageModeling, Trainer,\n",
    "                          TrainingArguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_all(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)\n",
    "\n",
    "# Для повторяемости результатов\n",
    "SEED = 42\n",
    "seed_all(SEED)\n",
    "# Загрузка переменных из .env файла\n",
    "load_dotenv()\n",
    "# Чтение токена\n",
    "token = os.getenv(\"HUGGING_FACE_ACCESS_TOKEN\")\n",
    "# Авторизация в Hugging Face\n",
    "login(token)\n",
    "# Отключим вывод предупреждений\n",
    "warnings.filterwarnings('ignore')\n",
    "# Устройство для тензорных вычислений и хранения модели в памяти.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Используем `FP16` - половинную точность (half-precision) для весов модели. Это позволяет экономить память GPU без существенного снижения качества обучения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1bd18d0be0445c9945366ed3ae4578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/gemma-2-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Половинная точность\n",
    "    token=token,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дообучение LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LoRA` (Low-Rank Adaptation of Large Language Models) - метод, который позволяет значительно сократить сложность и время дообучения больших языковых моделей  (LLM), за счёт заморозки матрицы весов. Вместо неё создаются и обучаются две малые матрицы весов низкой размерности. Итоговый результат получается сложением выходов новых матриц и замороженной матрицы. \n",
    "\n",
    "Метод основан на гипотезе, что для задачи дообучения LLM большинство параметров являются избыточными. Говорят, что при дообучении LLM имеет низкий внутренний ранг (intrinsic rank)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество обучаемых параметров до применения LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Параметров: 2 614 341 888\n",
      "Обучаемых параметров: 2 614 341 888\n"
     ]
    }
   ],
   "source": [
    "def count_params(model):\n",
    "    def pretty_number(num):\n",
    "        return \"{:,}\".format(num).replace(\",\", \" \")\n",
    "\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    grad_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Параметров:\", pretty_number(all_params))\n",
    "    print(\"Обучаемых параметров:\", pretty_number(grad_params))\n",
    "\n",
    "count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим LoRA к модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_RANK = 6\n",
    "\n",
    "# Настройки LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,  # Ранг малых матриц, веса которых мы будем обучать\n",
    "    lora_alpha=LORA_RANK,  # (alpha / r) - множитель для выходов малых обучаемых матриц\n",
    "    lora_dropout=0.1,  # Dropout регуляризация для малых обучаемых матриц\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество обучаемых параметров после применения LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Параметров: 2 615 539 968\n",
      "Обучаемых параметров: 1 198 080\n"
     ]
    }
   ],
   "source": [
    "count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что общее количество весов модели выросло, как раз на количество обучаемых весов.\n",
    "\n",
    "Примечание:\n",
    "\n",
    "* Хотя мы обучаем всего около 1,2 млн параметров с помощью LoRA, скорость обучения будет примерно такой же, как если бы мы обучали модель с 200 млн параметров без LoRA. Это происходит потому, что на этапе прямого хода всё равно требуется использовать все 2,6 млрд параметров базовой модели для вычисления выходов, и только на этапе обратного распространения ошибки начинает работать оптимизация LoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Очистим данные.\n",
    "* Возьмём только подвыборку из первых 15000 строк. Это уменьшит скорость обучения в 33 раза и позволит проверить гипотезы, подобрать гиперпараметры, проверить генерацию ответов моделью на известных ей входных данных. Обучение же модели на всех данных может занять сутки и более на GPU уровня T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['address', 'name_ru', 'rating', 'rubrics', 'text'],\n",
       "    num_rows: 15000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/prepared/data.csv\")\n",
    "df[\"name_ru\"] = df[\"name_ru\"].fillna(\"\")\n",
    "df[\"text\"] = df[\"text\"].str.replace(\"\\\\n\", \" \")\n",
    "df = df.drop_duplicates(ignore_index=True)\n",
    "df_slice = df[:15000]\n",
    "\n",
    "dataset = Dataset.from_pandas(df_slice)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Мы добавили в конец входного текста токен \\<eos> (End of sequence). Это поможет модели на этапе инференса «понять», когда нужно остановиться и не испортить сгенерированный отзыв, продолжая текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2499f36fd9e34f8f9caf337f83a2ce0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 15000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 128  # Максимальная длина текстов после токенизации\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Формируем входной текст\n",
    "    inputs = [\n",
    "        f\"Аddress: {address}\\nName: {name}\\nRating: {rating}\\nKeywords: {rubrics}\\nReview: {text}{tokenizer.eos_token}\"\n",
    "        for address, name, rating, rubrics, text in zip(\n",
    "            examples[\"address\"],\n",
    "            examples[\"name_ru\"],\n",
    "            examples[\"rating\"],\n",
    "            examples[\"rubrics\"],\n",
    "            examples[\"text\"],\n",
    "        )\n",
    "    ]\n",
    "    # Векторизуем входной текст и приводим его к выбранной длине\n",
    "    return tokenizer(\n",
    "        inputs, max_length=MAX_LENGTH, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "# Применяем препроцессинг\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=dataset.column_names\n",
    ")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 13500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=SEED)\n",
    "split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Гиперпараметры обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего шагов обучения: 10125\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM = 1\n",
    "N_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "n_steps = int(round(N_EPOCHS * len(split_dataset[\"train\"]) / BATCH_SIZE / GRAD_ACCUM, 0))\n",
    "print(\"Всего шагов обучения:\", n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/gemma-2-2b-lora-finetuned\",  # Директория сохранения модели\n",
    "    overwrite_output_dir=True,  # Перезапись директории при каждом запуске обучения\n",
    "    save_strategy=\"epoch\",  # Сохраняем модель в конце каждой эпохи\n",
    "    load_best_model_at_end=True,  # Сохраняем лучшую по метрике модель в конце обучения\n",
    "    metric_for_best_model=\"loss\",  # Метрика для оценки модели\n",
    "    save_total_limit=1,  # Сохранить только одну модель\n",
    "    eval_strategy=\"epoch\",  # Оценивать модель в конце каждой эпохи\n",
    "    learning_rate=LEARNING_RATE,  # Скорость обучения\n",
    "    lr_scheduler_type=\"linear\",  # Линейное снижение скорости обучения между шагами\n",
    "    per_device_train_batch_size=BATCH_SIZE,  # Размер батча данных на этапе обучения\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # Размер батча данных на этапе оценивания\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,  # Накапливать градиенты N шагов и подстроить веса\n",
    "    num_train_epochs=N_EPOCHS,  # Количество эпох\n",
    "    weight_decay=WEIGHT_DECAY,  # Коэффициент регуляризации\n",
    "    fp16=True,  # Использовать формат чисел FP16 (Floating point 16-bit)\n",
    "    logging_steps=500,  # Через сколько шагов выводить логи\n",
    "    report_to=\"none\",  # Выводить логи только в стандартный вывод\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объект, который извлекает данные батчами для обучения\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Отключаем режим маскирования текста MLM (Masked language modeling)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa6cde15fac4ebdb2fed591256d1ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0711, 'grad_norm': 0.8655328750610352, 'learning_rate': 0.00047530864197530866, 'epoch': 0.15}\n",
      "{'loss': 1.98, 'grad_norm': 0.8707149624824524, 'learning_rate': 0.00045066666666666665, 'epoch': 0.3}\n",
      "{'loss': 1.9586, 'grad_norm': 0.7667140364646912, 'learning_rate': 0.0004259753086419753, 'epoch': 0.44}\n",
      "{'loss': 1.941, 'grad_norm': 0.714769184589386, 'learning_rate': 0.00040128395061728395, 'epoch': 0.59}\n",
      "{'loss': 1.9204, 'grad_norm': 0.7591819167137146, 'learning_rate': 0.0003765925925925926, 'epoch': 0.74}\n",
      "{'loss': 1.9158, 'grad_norm': 0.817223072052002, 'learning_rate': 0.00035190123456790124, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d938b9df0efb423781016dc579dacdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8898636102676392, 'eval_runtime': 54.9415, 'eval_samples_per_second': 27.302, 'eval_steps_per_second': 6.825, 'epoch': 1.0}\n",
      "{'loss': 1.8951, 'grad_norm': 0.984058678150177, 'learning_rate': 0.00032725925925925924, 'epoch': 1.04}\n",
      "{'loss': 1.8463, 'grad_norm': 0.9858825206756592, 'learning_rate': 0.0003025679012345679, 'epoch': 1.19}\n",
      "{'loss': 1.8379, 'grad_norm': 1.5879651308059692, 'learning_rate': 0.00027792592592592593, 'epoch': 1.33}\n",
      "{'loss': 1.8382, 'grad_norm': 1.0335925817489624, 'learning_rate': 0.0002532345679012346, 'epoch': 1.48}\n",
      "{'loss': 1.8281, 'grad_norm': 0.8982611298561096, 'learning_rate': 0.0002285432098765432, 'epoch': 1.63}\n",
      "{'loss': 1.8406, 'grad_norm': 1.0581979751586914, 'learning_rate': 0.00020385185185185184, 'epoch': 1.78}\n",
      "{'loss': 1.8204, 'grad_norm': 0.994371235370636, 'learning_rate': 0.0001791604938271605, 'epoch': 1.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9af90ab2bed42eb811d1a6e4f182aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8506786823272705, 'eval_runtime': 52.6636, 'eval_samples_per_second': 28.483, 'eval_steps_per_second': 7.121, 'epoch': 2.0}\n",
      "{'loss': 1.7952, 'grad_norm': 0.93693608045578, 'learning_rate': 0.00015446913580246914, 'epoch': 2.07}\n",
      "{'loss': 1.7648, 'grad_norm': 0.9951069951057434, 'learning_rate': 0.00012977777777777779, 'epoch': 2.22}\n",
      "{'loss': 1.7472, 'grad_norm': 1.2637635469436646, 'learning_rate': 0.00010508641975308642, 'epoch': 2.37}\n",
      "{'loss': 1.7589, 'grad_norm': 1.1654475927352905, 'learning_rate': 8.044444444444444e-05, 'epoch': 2.52}\n",
      "{'loss': 1.7515, 'grad_norm': 0.9364428520202637, 'learning_rate': 5.575308641975309e-05, 'epoch': 2.67}\n",
      "{'loss': 1.7438, 'grad_norm': 1.1637076139450073, 'learning_rate': 3.106172839506173e-05, 'epoch': 2.81}\n",
      "{'loss': 1.7527, 'grad_norm': 0.9458349347114563, 'learning_rate': 6.4197530864197525e-06, 'epoch': 2.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c7889b66334f0fa0caf4eeaf31ac1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.838716745376587, 'eval_runtime': 51.3445, 'eval_samples_per_second': 29.214, 'eval_steps_per_second': 7.304, 'epoch': 3.0}\n",
      "{'train_runtime': 3318.883, 'train_samples_per_second': 12.203, 'train_steps_per_second': 3.051, 'train_loss': 1.8494055507330247, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10125, training_loss=1.8494055507330247, metrics={'train_runtime': 3318.883, 'train_samples_per_second': 12.203, 'train_steps_per_second': 3.051, 'total_flos': 6.3007869468672e+16, 'train_loss': 1.8494055507330247, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "\n",
    "* Видим, что в конце обучения метрика `eval_loss` (Кросс-энтропия) всё ещё уменьшается. Следовательно, модель ещё не дообучена.\n",
    "* Можно увеличить количество эпох, или немного увеличить скорость обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка генерации текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим запрос (prompt) из данных на которых модель не обучалась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аddress: Санкт-Петербург, проспект Художников, 27, корп. 1\n",
      "Name: КрасКи\n",
      "Rating: 5\n",
      "Keywords: Салон красоты\n",
      "Review:  Впервые побывала в этой студии, делала стрижку. Сразу видно, что студия новая, очень уютная и чистая. Мастер приятная девушка, все сделала быстро и сразу поняла чего я хочу. Обязательно вернусь снова! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Возьмём векторизованный текст из тестового датасета\n",
    "vectorized_text = split_dataset[\"test\"][2][\"input_ids\"]\n",
    "# Декодируем из вектора обратно в текст\n",
    "text = tokenizer.decode(vectorized_text, skip_special_tokens=True)\n",
    "# Разделим текст на запрос и эталонный ответ\n",
    "prompt, text = text.split(\"Review: \")\n",
    "prompt = prompt + \"Review: \"\n",
    "print(prompt, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аddress: Санкт-Петербург, проспект Художников, 27, корп. 1\n",
      "Name: КрасКи\n",
      "Rating: 5\n",
      "Keywords: Салон красоты\n",
      "Review: 25 лет я хожу в салон КрасКи на Проспекте Художников. Замечательные мастера, очень профессионалы, которые делают всё на высшем уровне. Приятная дружелюбная атмосфера. Спасибо вам большое!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Параметры генерации текста\n",
    "generation_config = {\n",
    "    \"max_length\": 128,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"do_sample\": True,\n",
    "    \"num_beams\": 1,\n",
    "    \"temperature\": 0.95,\n",
    "    \"top_k\": 10,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "# Векторизация текста\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "# Генерация текста\n",
    "outputs = model.generate(**inputs, **generation_config)\n",
    "response = outputs[0][outputs.shape[-1] :]\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "\n",
    "* Сгененрированный текст достаточно качественный. Есть небольшие ошибки в грамматике.\n",
    "* Сгененрированный текст не совпадает с истинным, но они и не должны совпадать. Мы использовали для генерации запрос, который модель раньше не видела. Поэтому возможно огромное количество генераций текстов, и все из них правильные, с точки зрения модели.\n",
    "* Если бы мы использовали для генерации уже известный модели запрос, то мы проверяли бы лишь степень переобученности модели, а не её обобщающую способность."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
